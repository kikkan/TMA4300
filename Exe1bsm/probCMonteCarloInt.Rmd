---
title: "Project 2"
author: "Erling og Christian"
date: "03 02 2022"
output: 
  bookdown::pdf_document2:
    toc_depth: '3'
    number_sections: false
  # pdf_document:
  # #   toc: no
  #   toc_depth: '3'
subtitle: Computer Intensive Statistical Methods
urlcolor: blue
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,
                      strip.white=TRUE,
                      prompt=FALSE,
                      cache=TRUE, 
                      size="scriptsize",
                      fig.width=5, 
                      fig.height=5, 
                      fig.align = "center")
```

# This is from Fab
```{r From fab, options}
generate_from_normal <- function(n)
{
  u1<-runif(n)
  u2<-runif(n)
  r<-sqrt(-2*log(u1))
  x_1<-2*pi*u2
  x=r*cos(x_1)
  return(x)
}
```

# Caption

## Problem C: Monte Carlo integration and variance reduction
\textcolor{blue}{Intro, do we need it?}\\ 
Here we will consider Monte Carlo integration to estimate $\theta = P(X>4)$ when $X \sim N(0,1)$. Then we will compare the variance reduction in importance sampling and antithethic sampling.

### 1 Monte Carlo integration {#C1}
Let $h(X) = I(X>4)$, where $I$ is the indicator function, such that
$$
\begin{split}
  E[h(X)] &= \int^{\infty}_{-\infty} h(x) f(x) dx \\
  &= \int^{\infty}_{-\infty} I(x>4)f(x)dx \\
  &= P(X>4) \\
  &= \theta.
\end{split}
$$
Then, the Monte Carlo Estimate of $\theta$ is given by
$$
\hat \theta_{MC} = \frac1n \sum^{n}_{i=1} h(x_i).
$$
Now we will find a $1-\alpha$ confidence interval for $\theta$ based on our sample set. First we need the expected value of the Monte Carlo estimator
$$
\begin{aligned}
  E[\hat \theta] &= E[\frac{1}{n} \sum_{i=1}^n h(x_i)] \\
  &= \frac{1}{n} \sum^{n}_{i=1}{\theta} \\
  &= \theta,
\end{aligned}
$$
and it's variance
$$
\begin{aligned}
  Var(\hat \theta) &= Var(\frac1n \sum^{n}_{i=1}{h(x_i)})  \\
  &= \frac{1}{n^2} \sum^{n}_{i=1}{Var(h(x_i))} \\
  &= \frac1n \frac{1}{n-1} \sum^{n}_{i=1}{(h(x_i) - \hat \theta)^2}.
\end{aligned}
$$
Then we get the statistic
$$
T_{MC} = \frac{\hat \theta_{MC} - \theta}{\sqrt{\hat {Var}(\hat\theta_{MC})}} \sim t_{n-1}.
$$
Below there is an implementation with $n=100000$ samples of $X$ which we use to find the Monte Carlo estimate $\theta$. \textcolor{red}{Is there something wrong with this variance?} 
```{r C1_MCconfint, options}
set.seed(321)                       # Seed for reproducibility.
n        = 1e5
x        = generate_from_normal(n)  # Drawing n samples fron N(0,1)
h        = x>4
MCest    = mean(h)                  # Monte Carlo estimate
theta    = pnorm(4, lower.tail = F) # True theta
                                    # Confidence interval and results
svMC     = sum((h - MCest)^2)/(n*(n-1)) # Sample Variance
alpha    = 0.05
t        = qt(alpha/2, n-1, lower.tail = F) # (1-alpha) significance
lwrUpr   = sqrt(svMC)*t # lower and upper deviation from mean
ciMC     = MCest + c(-lwrUpr, lwrUpr)
resultMC = c(Estimator= MCest, 
             Confint  = ciMC, 
             Var      = svMC, 
             error    = abs(theta-MCest))
resultMC
theta
```
Here we see and error in the $1\cdot10^{-5}$ decimal and that the true value coincide with the $95\%$ confidence interval.


### 2 Importance sampling {#C2}
Here we will use importance sampling on the same problem as in [C1](#C1) to try to reduce the variance of the Monte Carlo integration. The proposal distribution is
$$
g(x) = 
\begin{cases}
  cxe^{-x^2/2} &, \ x>4 \\
  0 &,\ \text{otherwise},
\end{cases}
$$
where $c$ is a normalizing constant. Now, let $x_1,...,x_n \stackrel{i.i.d}{\sim} g(x)$ and let $w_i = w(x_i) = f(x_i)/g(x_i) = f_i/g_i$ be the weights. \textcolor{red}{(where $w_i, f_i$ and $g_i$ are function evaluations at $x_i$)}. Then, the importance sampling estimator of $\theta$ is
$$
\hat \theta_{IS} = \frac{ \sum^{n}_{i=1}{h_i w_i}}{n}. 
$$
In order to use inversion sampling on the proposal distribution $g$ we need its cdf
$$
\begin{split}
  G(x) &= \int^{x}_{4}{cye^{-y^2/2}}dy \\ 
  &= \int^{x^2/2}_{8}{ce^{-u}du} \\
  &=\left[ -ce^{-u}\right]_{8}^{x^2/2} \\
  &= c(e^{-8} - e^{-x^2/2}).
\end{split}
$$
Since $g$ is a distribution, and therefore $\int^{\infty}_{4}{g(x)}dx = 1$,  we can find $c$ by solving
$$
\begin{split}
  \left. c(e^{-8} - e^{-x^2/2}) \right |_{x=\infty} &=1\\
  c &= e^{8}.
\end{split}
$$
Then we have $G(x) = 1 - e^{8-x^2/2}$. Now we can sample from $g$ by solving $U=G(x)\sim Unif(0,1)$ for $x$, that is,
$$
\begin{split}
  U &= 1 - e^{8-x^2/2} \\
  -2ln(1-U) &= x^2 - 16 \\
  x &= \sqrt{16 - 2ln(1-U)}.
\end{split}
$$
Thus, our samples are generated by inserting randomly selected $U\sim Unif(0,1)$ admits samples from $X\sim g$.

We also need the expected value,
$$
\begin{split}
  E[\hat \theta_{IS}] &= E \left[\frac{ \sum^{n}_{i=1}{h_i w_i}}{n} \right] 
  \\
  &= \frac{1}{n} \sum^{n}_{i=1}{ \int^{\infty}_{0}{h_i \frac{f_i}{g_i} g_i dx}} 
  \\
  &= \frac1n \sum^{n}_{i=1} \int^{\infty}_{0} h_i f_i dx \\
  &= \frac1n \sum^{n}_{i=1} E[h_i] \\
  &= \frac1n n\theta \\
  &= \theta,
\end{split}
$$
and the sample variance,
$$
\begin{split}
  Var(\hat \theta_{IS}) &= Var \left(\frac{ \sum^{n}_{i=1}{h_i w_i}}{n} \right) 
  \\
  &= \frac{1}{n(n-1)} \sum^{n}_{i=1} \left( h_i w_i - \sum^{n}_{i=1} \frac{h_iw_i}{n} \right)^2 
  \\
  &= \frac{1}{n(n-1)} \sum^{n}_{i=1}\left( h_i w_i - \hat \theta_{IS}\right)^2,
\end{split}
$$
of the importance sample estimator to compute the $(1-\alpha)%$ confidence interval. Below we have implemented the computation of the importance sample estimate along with the $95\%$ confidence interval.
```{r expIS, options}
expSampler <- function(n){
  # Samples from proposal distribution g
  u = runif(n)
  return(sqrt(16-2*log(1-u)))
}

w <- function(x) {
  # Weight function
  f = dnorm(x)
  g = ifelse(x>4, x*exp(8-0.5*x^2),0)
  return(f/g)
}
set.seed(321)
gx        = expSampler(n) # Sample from proposal
gh        = (gx>4)*1
ISest     = mean(gh*w(gx))
svIS      = sum((gh*w(gx) - ISest)^2)/(n*(n-1))
ISconfint = ISest + c(-t*sqrt(svIS), t*svIS)
# Results
resultIS  = c(ISestimate=ISest, confint=ISconfint, var=svIS, error = abs(theta - ISest))
results   = rbind(MC=resultMC, IS=resultIS)
results
theta
```
Here we see that importance sampling has reduced the variance by a factor of $Var(\hat\theta_{MC}) / Var(\hat \theta_{IS}) =$ `svMC/svIS` $=$ `r svMC/svIS`. Also, the importance sample estimator is a much more precise estimate.

\textcolor{red}{Not sure how to compute how many more samples we would need, since the error influences.} 

### 3 Antithetic Sampling
Now we will combine the importance sampling with the use of antithetic variates. We start by modifying the sample generator for $g$ in task [C2](#C2) so that it produces $n$ pairs, $X=x_i$ and $Y=y_i$, of antithetic variates by using $u_i$ and $1-u_i$ as input to $G^{-1}$, respectively.
```{r C3antitheticSampler, options}
expSamplerAnti <- function(n) {
  # Pairwise sampling from proposal distribution
  # evaluated at u and 1-u.
  u = runif(n) 
  return(data.frame(x=sqrt(16-2*log(1-u)),
                    y=sqrt(16-2*log(u))))
}
# xy <- expSamplerAnti(n)
# gx <- ifelse(xy$x>4, xy$x*exp(8-0.5*xy$x^2),0)
# gy <- ifelse(xy$y>4, xy$y*exp(8-0.5*xy$y^2),0)
# plot(xy$x,gx)
# points(xy$y,gy, col="cyan3",cex=1)
```
Then, the importance sample estimates for each of the pairs are
$$
\begin{split}
  \hat \theta_X &= \frac1n \sum^{n}_{i=1} h(x_i)w(x_i),\\
  \hat \theta_Y &= \frac1n \sum^{n}_{i=1} h(y_i)w(y_i),
\end{split}
$$
and the antithetic sample estimator is
$$
\hat \theta_A = \frac{\hat \theta_X + \hat \theta_Y}{2}. 
$$
We also need the expected value \textcolor{red}{Should we show that each expectation is theta?} 
$$
\begin{split}
  E[\hat \theta_{AS}] &= E\left[ \frac{\hat \theta_X + \hat \theta_Y}{2} \right]
  \\
  &= \frac{1}{2n} \sum^{n}_{i=1} (E[h(x_i)w(x_i)] + E[h(y_i)w(y_i)]) \\
  & \stackrel{*}{=}  \frac{1}{2n} \sum^{n}_{i=1} 2\theta \\
  &= \theta,
\end{split}
$$
where $*$ since the proposal distribution evaluations cancel in each expectation. \textcolor{red}{not true or not needed?}. The variance of the estimator is
$$
\begin{split}
  Var(\hat \theta_{AS}) &= \frac14(Var(\hat \theta_X) + Var(\hat \theta_Y) + 2Cov(\hat \theta_X,\hat \theta_Y))
  \\
  \Downarrow& \quad Var(\hat \theta_X) = Var(\hat \theta_Y) \\
  &= \frac{(1+\rho_{XY}) S^2_{XY}}{2n}, 
\end{split}
$$
where $\rho_{XY}=Cov(\hat \theta_X,\hat \theta_Y)$ and $S^2_{XY}$ is the sample variance of either estimator $\hat\theta_X$ or $\hat\theta_Y$.
```{r C3Antithetic, options}
set.seed(321)
n = 5e4
xy = expSamplerAnti(n)
hxy = (xy>4)*1
hwx = hxy[,"x"]*w(xy$x)
hwy = hxy[,"y"]*w(xy$y)
hwxy = (hwx + hwy)/2
ASest = mean(hwxy)
var(hwxy)
svAS = (var(hwx) + var(hwy) + 2*cov(hwx,hwy))/4
lwrUprAS = c(-t,t)*sqrt(svAS)
confintAS = ASest + lwrUprAS
resultAS <- c(ASest, confintAS, svAS, abs(theta - ASest))
rbind(results, AS=resultAS)
```
 
