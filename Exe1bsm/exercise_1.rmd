---
title: "Project 1"
author: "Erling Fause Steen og Christian Oppeg√•rd Moen"
date: "03 02 2022"
output: 
  bookdown::pdf_document2:
    toc_depth: '3'
    number_sections: false
  # pdf_document:
  # #   toc: no
  #   toc_depth: '3'
subtitle: Computer Intensive Statistical Methods
urlcolor: blue
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,
                      strip.white=TRUE,
                      prompt=FALSE,
                      cache=TRUE, 
                      size="scriptsize",
                      fig.width=7, 
                      fig.height=5, 
                      fig.align = "center")
```

## Problem A: Stochastic simulation by the probabilty integral transform and bivariate techniques


### 1 Exponential Distribution
 
We are going to generate $n$ samples from an exponential distribution with rate parameter $\lambda.$ 
Let $X \sim Exp(\lambda)$. This gives pdf and cdf 
$$
f(x)=\lambda \exp(-\lambda x)
$$
$$
F(x)=1-\exp(-\lambda x)
$$
for $X$.
The inversion method can be used to generate samples from the exponential distribution. Firstly, the random variable $U$ is generated from the standard uniform distribution in interval $[0,1].$ Then $X=F^{-1}(U)$. The algorithm is then
$$
u \sim U[0,1]
$$
$$
x=-\frac{1}{\lambda} \log(u)
$$
$$
\text{return } x
$$
This algorithm is implemented below.
```{r}
generate_exponential <- function(n, lambda)
{
  #generate samples from U
  u <-runif(n)
  #calculate x
  x <- -(1/lambda) * log(u)
  return(x)
}
```
Below, $n=10000$ samples are generated with $\lambda=3$. The result is plotted against the theoretical distribution.

```{r}
library(ggplot2)
n=100000
lambda=3
#generate samples from the function
generate_f_1=generate_exponential(n, lambda)
#Plot the generated samples
ggplot()+
  geom_histogram(
    data=as.data.frame(generate_f_1),
    mapping=aes(x=generate_f_1,y=..density..),
    binwidth=0.01
#Plot the theoretical density    
  )+
  stat_function(
  fun=dexp,
  args=list(rate=lambda),
  aes(col="Theoretical distribution"))+
  xlim(0,3)+
  xlab("x")+
  ggtitle("Generated samples from exponential distribution")
```
The histogram of the simulated values seems to be pretty close to the theoretical distribution. We also check the empirical mean and variance and compare it to the theoretical mean and variance. The theoretical mean and variance is given by
$$
E(X)=\frac{1}{\lambda}=\frac{1}{3} \ \ Var(X)=\frac{1}{\lambda^2}=\frac{1}{9}
$$
```{r}
est_mean=mean(generate_f_1)
est_var=var(generate_f_1)
```
The estimated mean is `r est_mean` and the estimated variance is `r est_var`. Both values are close to the theoretical.

### 2 Sampling from function g(x)

a)

We want to find the cumulative distribution function and the inverse of the cumulative distribution function when the probability density function is given by

\begin{equation} \label{eq:gx}
  g(x) =
  \begin{cases}
    c x^{\alpha-1} & \text{if } 0 < x < 1,
    \\
    ce^{-x} & \text{if } 1 \leq x, \\
    0 & \text{otherwise},
  \end{cases}
\end{equation}


The cumulative distribution $G(x)$ is given by
$$
G(x)=\int_{-\infty}^{x} g(t) dt
$$
Thus, for $0<x<1,$ the cdf becomes

$$
G(x)=\int_{0}^{x} c t^{\alpha-1}=c \cdot \left [\frac{1}{\alpha} t^{\alpha}  \right ]_{0}^{x}
=  \frac{c}{\alpha} x^{\alpha}.
$$
For $1 \leq x$, the cdf is given by

$$
G(x)=\int_{0}^{1}ct^{\alpha-1}+ \int_{1}^{x} c e^{-t} dt=c \cdot \left [\frac{1}{\alpha} t^{\alpha}  \right ]_{0}^{1}+c \cdot \left [ - e^{-t}\right]_{1}^{x}=\frac{c}{\alpha}-ce^{-x}+e^{-1}=c \cdot \left( \frac{1}{\alpha}-e^{-x}+\frac{1}{e} \right)
$$
The constant $c$ can be found by solving the following equation for $c$,

$$
\int_{0}^{1}ct^{\alpha-1}+\int_{1}^{\infty}ce^{-t}dt=1 
$$
$$
\frac{c}{\alpha}+c \cdot\left [-e^{-t} \right]_{1}^{\infty}=\frac{c}{\alpha}+\frac{c}{e} \implies \frac{c}{\alpha}+\frac{c}{e}=1 \ \ \implies  c=\frac{\alpha e}{e+\alpha}.
$$
Thus, the cumulative distribution function is

\begin{equation} \label{eq:gx}
  G(x) =
  \begin{cases}
    \frac{e}{e+\alpha} x^{\alpha} & \text{if } 0 < x < 1,
    \\
    \frac{e}{e+\alpha} -\frac{\alpha e^{1-x}}{e+\alpha}+\frac{\alpha}{e+\alpha} & \text{if } 1 \leq x, \\
    0 & \text{otherwise},
  \end{cases}
\end{equation}

The inverse of this cumulative distribution function can be found by solving $y=G(x)$ for $x$.

For $0<x<1$, we have

$$
y=\frac{ex^{\alpha}}{e+\alpha} \implies y(e+\alpha)=ex^{\alpha} \implies x=\left (\frac{u(e+\alpha)}{e} \right )^{\frac{1}{\alpha}}
$$
Thus, the inverse of the cumulative distribution function is 

$$
G^{-1}(y)=\left ( \frac{y(e+\alpha)}{e} \right)^{\frac{1}{\alpha}} 
$$
for $0<G^{-1}(y)<1 \implies 0 < y <\frac{e}{e+\alpha}.$ For $1 \leq x$, the following eqaution is solved for $x$

$$
y=1-\frac{\alpha e^{-x+1}}{e+\alpha} \implies x=1-\ln \left(\frac{(1-y)(e+\alpha)}{\alpha} \right)=\ln \left (\frac{\alpha e}{(1-y)(e+\alpha)} \right) 
$$
$$
\implies G^{-1}(y)=\ln \left (\frac{\alpha e}{(1-y)(e+\alpha)} \right)
$$


When $x=\infty$, we have $y=1.$ Therefore the inverse cumulative function is 

\begin{equation} \label{eq:gx}
  G^{-1}(y) =
  \begin{cases}
    \left (\frac{y(e+\alpha)}{e} \right)^{1/\alpha} & \text{if } 0 < y < \frac{e}{e+\alpha},
    \\
    \ln \left (\frac{\alpha e}{(1-y)(e+\alpha)} \right) & \text{if } \frac{\alpha}{\alpha +e} \leq y < 1, \\
    0 & \text{otherwise},
  \end{cases}
\end{equation}

b)

The inversion method is used to generate samples from $g$.

```{r}
  sample_from_g = function(n, alpha) {
    #generate samples from unif(0,1)
    u<-runif(n)
    #x=G^(-1)(u)
    x<-vector()
    for(i in 1:n)
    {
      if(u[i]<((exp(1))/(exp(1)+alpha)))
      {
        x[i]=((u[i]*(exp(1)+alpha))/(exp(1)))^(1/alpha)
      }
      else if(u[i]>=((exp(1))/(exp(1)+alpha)))
      {
        x[i]=log((alpha*exp(1))/((1-u[i])*(exp(1)+alpha)))
      }
    }
    return(x)
  }
```

The result for $\alpha=0.8$ and $n=10000$ is plotted against the theoretical distribution.

```{r}
#The theoretical distribution
  density_g = function(x, alpha) {
    # The Normalizing constant
    c = (alpha*exp(1))/(alpha + exp(1))
    #Create an empty vector with the length equal to the number of x-values
    x_final = vector(length = length(x))
    #Fill in the values in g(x)
    for(i in 1:length(x))
    {
      if(x[i]>0 & x[i]<1)
      {
        x_final[i]<-c*x[i]^(alpha-1)
      }
      else if(1<=x[i])
      {
        x_final[i]<-c*exp(-x[i])
      }
    }
    return(x_final)
  }

n=1000000
alpha=0.80
sample_g<-sample_from_g(n,alpha)
ggplot()+
  geom_histogram(
    data=as.data.frame(sample_g),
    mapping=aes(x=sample_g,y=..density..),
    binwidth=0.01
    
  )+
  stat_function(fun=density_g,args=list(alpha=alpha), aes(col="Theoretical distribution"))+
  xlim(0,3)+xlab("x")+ggtitle("Generated samples from g(x)")
  
```
We see that the histogram gives a relatively similar plot as the theoretical distribution. 
```{r}
est_mean_g<-mean(sample_g)
est_var_g<-var(sample_g)
```

The empirical mean is `r est_mean_g` and the empirical variance is `r est_var_g`. 
The theoretical mean is given by
$$
E(X)=\int_{0}^{1} cx^{\alpha -1}xdx+\int_{1}^{\infty}c e^{-x}xdx=\frac{c}{\alpha+1}+\frac{2 \alpha}{\alpha+e}
=0.79
$$
which is close to our estimated empirical mean.
The theoretical variance is given by
$$
Var(X)=E(X^2)-E(X)^2 ,\ \ E(X^2)=\int_{0}^{1} cx^{\alpha-1}x^2 dx+\int_{1}^{\infty}c e^{-x} x^2 dx=c/(\alpha+2)+\frac{5 \alpha}{\alpha + e}=1.357
$$
$$
\implies Var(X)=0.7329.
$$

### 3 Sampling of function f(x) 

We consider the probability density function 

$$
f(x)=\frac{c e^{\alpha x}}{(1+e^{\alpha x})^2}, \ \ \infty <x< \infty, \ \ \alpha>0
$$

a) To find the normalizing constant, we consider the integral $I$ of the pdf over $\text{R}$

$$
I=\int_{-\infty}^{\infty}f(x) dx=1 \implies \int_{-\infty}^{\infty} \frac{c e^{\alpha x}}{(1+e^{\alpha x})^2}dx=1
$$
Let $u=1+e^{\alpha x}$. This means that $\frac{du}{dx}=\alpha e^{\alpha x}$ and $u(-\infty)=1$ and $u(\infty)=\infty$. By using variable change the integral becomes 
$$
I=\int_{1}^{\infty} \frac{c}{\alpha} u^{-2}du=\frac{c}{\alpha} \left [-u^{-1} \right ]_{1}^{\infty}=\frac{c}{\alpha}
$$

$$
I=1 \implies \frac{c}{\alpha}=1 \implies c=\alpha
$$
The pdf is therefore 

$$
f(x)=\frac{\alpha e^{\alpha x}}{(1+e^{\alpha x})^2}.
$$

b)
The cumulative distribution function is given by

$$
F(x)=\int_{-\infty}^{x} \frac{\alpha e^{\alpha t}}{(1+ e^{\alpha t})^2}dt.
$$
By using $u=1+e^{\alpha t}$, we get 

$$
F(x)=\int_{-\infty}^{1+e^{\alpha x}} u^{-2} du= \left [-u^{-1} \right]_{1}^{1+e^{\alpha x}}=  \frac{-1}{1+e^{\alpha x}}+1=\frac{e^{\alpha x}}{1+e^{\alpha x}}
$$
The inverse cumulative distribution is found by solving $y=F(x)$ for $x.$

$$
y=\frac{e^{\alpha x}}{1+e^{\alpha x}} \implies e^{\alpha x}=\frac{y}{1-y} \implies x=\frac{1}{\alpha} \ln \left( \frac{y}{1-y} \right)
$$
This means that the inverse cumulative distribution function is 
$$
F^{-1}(y)=\frac{1}{\alpha} \ln \left( \frac{y}{1-y} \right)
$$
 c)
In the following chunk there is code for a function generating samples form $f$ by using the inversion method

```{r}
generate_f <- function(n, alpha)
{
  #sample from Uniform(0,1)
  u<-runif(n)
  #x=F^(-1)(y)
  x<-(1/alpha)*log(u/(1-u))
  return(x)
}
```
To check that the function works properly, an example with using the function $\alpha=2$ and $n=1000000$ is plotted against the theoretical distribution.

```{r}
#Theoretical distribution
theoretical_f <- function(x, alpha) {
return(alpha * exp(alpha * x) / (1 + exp(alpha * x))^2)
}

```


```{r}
library(ggplot2)
n=1000000
alpha=2
sample_f <- generate_f(n,alpha)
ggplot()+
  #Plotting a histogram of the samples
  geom_histogram(data=as.data.frame(sample_f),
                 mapping=aes(x=sample_f,y=..density..),
                 binwidth=0.01)+
  #Plotting the theoretical distribution
  stat_function(fun=theoretical_f,
                args=list(alpha=alpha),
                aes(col="Theoretical distribution"))+
  xlab("x")+ 
  xlim(-4,4)+
  ggtitle("Generated samples from f(x)")
```

The theoretical mean and variance of $X \sim f(x)$ is given by
$$
E(X)=\int_{-\infty}^{\infty}\frac{c e^{\alpha x}}{(1+e^{\alpha x})^2}x dx=0
$$
$$
Var(X)=E(X^2)-E(X)^2=E(X^2)
$$
We estimate $E(X^2)$
```{r}
theo_var_f<-integrate(function(x)(x^2*theoretical_f(x,alpha)),-100,100)$val
theo_var_f
```
We also find the empirical mean and variance
```{r}
mean_f<-mean(sample_f)
var_f<-var(sample_f)
```

the empirical mean is `r mean_f` and the empirical variance is `r var_f`. Both values are close to the theoretical values. 

### 4 Standard normal distribution

We use the Box-Muller algorithm to represent independent variables which are standard normal distributed. Let $X \sim N(0,1)$ and $Y \sim N(0,1)$ be independent. The joint distribution of these two variables is 
$$
f(x,y)=f_{X}(x) \cdot f_{Y}(y)=\frac{1}{2 \pi}e^{-\frac{x^2+y^2}{2}}
$$
By using polar coordinates where $x^2+y^2=r^2$, the joint distribution becomes
$$
f(r)=\frac{1}{2 \pi}e^{-\frac{r^2}{2}}.
$$
This is a joint distribution of $r^2 \sim \exp(1/2)$
and $X_1 \sim \text{Unif}(0,2 \pi).$ This means that 
$$
X=r \cos (X_1)
$$
$$
Y=r \sin(X_1)
$$
are normal distributed. If we have $U_1 \sim \text{Uniform}(0,1)$ and $U_2 \sim\text{Uniform}(0,1)$, then we have $r = \sqrt{-2 \log(u_1)}$ and $X_2 = 2 \pi u_2$

In the following chunk, the Box-Muller algorithm is implemented. We first draw two different samples from $\text{Unif}(0,1).$ We then calculate $r$ and $X_1$ and at last return $X=r \cos(X_1)$ which is standard normal distributed.

```{r}
generate_from_normal <- function(n)
{
  #generate samples from u1 and u2
  u1<-runif(n)
  u2<-runif(n)
  #calculate r
  r<-sqrt(-2*log(u1))
  #calculate x_1
  x_1<-2*pi*u2
  x=r*cos(x_1)
  return(c(x))
}
#Generating 1000000 samples from the normal distribution
n=1000000
sample_normal<-generate_from_normal(n)
ggplot()+
  #Plotting the samples
  geom_histogram(
    data=as.data.frame(sample_normal),
    mapping=aes(x=sample_normal, y=..density..),
    binwidth=0.01
  )+
  #Plotting the standard normal distribution
  stat_function(
    fun=dnorm,
    args=list(mean=0,sd=1),
    aes(col="Theoretical distribution"))+ggtitle("Samples from the standard normal distribution")
  
```

We calculate the empirical mean and the empirical variance of the samples.

```{r}
mean_std_normal<-mean(sample_normal)
var_std_normal<-var(sample_normal)
```

We expect the mean and variance to be $0$ and $1$ respectively. The empirical mean and variance are `r mean_std_normal` and `r var_std_normal` respectively, which is close to what we expected.


### 5 d-variate normal distribution

We want to to simulate from a d-variate normal distribution with mean vector $\mu$ and covariance matrix $\Sigma.$ . Let $x \sim \text{Normal}(0,I_d)$, where $I_d$ is the identity matrix. Then

$$
y=\mu+DZ \sim Normal(\mu,DD^T)
$$
Thus, we have to find $D$ such that $\Sigma=DD^T.$
The Cholesky decomposition can be used to find $D.$

```{r}
generate_d_normal <-function(n,mu,cov,d)
{
  #D is the cholesky decomposition of the covariance matrix
  D<-t(chol(cov))
  x<-generate_from_normal(d*n)
  X<-matrix(x, nrow=d, ncol=n)
  y<-mu+D %*% X
  return(y)
}
```
the covariance matrix we use to test the function is

$$
\left(\begin{array}{ccc} 
3 & -1&1\\
-1 & 4&-1 \\
1&-1&3
\end{array}\right)
$$

and the mean vector is 
$\mu=[1,7,2]^T$.
```{r}
n<-10000
mu<-c(3,4,5)
cov_mat<-cbind(c(3,-1,1), c(-1,4,-1), c(1,-1,3))
cov_mat
sample_normal_d<-generate_d_normal(n,mu,cov_mat,3)
```
We check whether the function generates good estimates of the mean vector and the covariance matrix.

```{r}
mu_hat<-rowMeans(sample_normal_d)
cov_mat<-cov(t(sample_normal_d))
mu_hat
cov_mat
```
As seen in the chuck above, the algorithm generates pretty good estimated of $\mu$ and $\Sigma$.


## Problem B: The gamma distribution

### 1 Rejection sampling
The gamma distribution with parameters $\alpha \in (0,1)$ and $\beta=1$ has probability density function

\begin{equation} \label{eq:gx}
  f(x) =
  \begin{cases}
    \frac{1}{\Gamma (\alpha)} x^{\alpha-1} e^{-x} & \text{if } 0 < x,
    \\
    0 & \text{otherwise},
  \end{cases}
\end{equation}

a)
The acceptance probability $\gamma$ in the rejection sampling algorithm is given by 

$$
\gamma=c^{-1} \cdot \frac{f(x)}{g(x)}
$$
where $g(x)$ is the proposal density. We use the density in problem A.2. 

For $0<x<1$, 
$$
g(x)=\frac{\alpha e}{\alpha +e} x^{\alpha-1}.
$$
This means that the acceptance probability is

$$
\gamma=c^{-1} \frac{1}{\Gamma(\alpha)}x^{\alpha-1} e^{-x} \cdot \frac{\alpha+e}{\alpha e \cdot x^{\alpha-1}}=\frac{e^{-x}(\alpha+e)}{c\alpha e \Gamma(\alpha)}
$$
For $x \geq 1$, we have

$$
g(x)=\frac{\alpha e}{\alpha+e} e^{-x}
$$
which give the acceptance probability

$$
\gamma=\frac{x^{\alpha-1}(\alpha+e)}{c \Gamma(\alpha) \alpha e}
$$
To sample from $f$, we need to find an efficient bound $c$ such that 
$$
\frac{f(x)}{g(x)}\leq c, \forall x
$$
We need to choose the smallest possible value for $c$. 

For $0<x<1$
$$
 c \geq\frac{(\alpha+e)}{\alpha e \Gamma(\alpha)} \geq\frac{e^{-x}(\alpha+e)}{\alpha e \Gamma(\alpha)} =\frac{f(x)}{g(x)},
$$
and for $x \geq 1$

$$
c \geq\frac{(\alpha+e)}{\alpha e \Gamma(\alpha)} \geq\frac{x^{\alpha-1}(\alpha+e)}{ \Gamma(\alpha) \alpha e} =\frac{f(x)}{g(x)}.
$$
We can therefore choose $c=\frac{(\alpha+e)}{\alpha e \Gamma(\alpha)}.$ The acceptance probability is therefore
\begin{equation} \label{eq:gx}
  \gamma =
  \begin{cases}
    x^{\alpha -1} & \text{if } x \geq 1,
    \\
    e^{-x} & \text{if } 0 < x< 1, 
  \end{cases}
\end{equation}

(b)
The rejection sampling algorithm is used to generate a vector of n independent samples from $f$. We use $n=100000$ and $\alpha=0.8$


```{r}
#Theoretical pdf of Gamma
theoretical_gamma<-function(x, alpha)
{
  if(0<x)
  {
    return((1/gamma(alpha))*x^(alpha-1)*exp(-x))
  }
  else if(x<=0)
  {
    return(0)
  }
  
}
sample_gamma<-function(n,alpha)
{
  x<-vector()
  #The c we found
  c<-(alpha+exp(1))/(alpha+exp(1))
  for (i in 1:n) {
    finished=0
    while(finished==0)
    {
      #sample from g in A.2
    xi<-sample_from_g(1,alpha)
    #samples from Unif(0,1)
    u<-runif(1)
    #The pdf of gamma with alpha in (0,1) and beta=1
    f<-theoretical_gamma(xi,alpha)
    #Pdf of g in A.2
    g<-density_g(xi,alpha)
    #Acceptance probability
    gamma<-(1/c)*(f/g)
    #Accept if u<=acceptance probability
    if(u<=gamma)
    {
      x[i]=xi
      finished=1
    }
    }
  }
  return(x)
}
#Generate 100000 samples with alpha=0.8
x_gamma<-sample_gamma(100000,0.8)
#Plot the samples
ggplot()+
  geom_histogram(
    data=as.data.frame(x_gamma),
    mapping=aes(x=x_gamma,y=..density..),
    binwidth=0.15
    #Plot the theoretical density
  )+
  stat_function(
    fun=dgamma,
    args=list(shape=0.8),
    aes(col="Theoretical distribution"))+
      ggtitle("Simulation of gamma distribution with alpha=0.8")+xlim(0,3)+ylim(0,1.5)+xlab("x")
```
The theoretical mean for a Gamma-distribution is $\alpha/ \beta$, which is $0.8$ in our case. The theoretical variance is $\alpha / \beta^2,$ which is also $0.8$ in our case. 
```{r}
emp_mean_gamma_1<-mean(x_gamma)
emp_var_gamma_1<-var(x_gamma)
```

The empirical mean and variance are `r emp_mean_gamma_1` and `r emp_var_gamma_1` respectively. Both are close to the theoretical values. 

### 2 Ratio of uniforms method
(a)
$a=\sqrt{\text{sup}_{x} f^{*}(x)},$ where 
\begin{equation} \label{eq:gx}
  f^{*}(x) =
  \begin{cases}
    x^{\alpha-1} e^{-x} & \text{if } 0 < x ,
    \\
    0 & \text{otherwise},
  \end{cases}
\end{equation}

We start to find $\text{sup}_{x} f^{*}(x).$ If $x<0,$ 
$$
\frac{\partial}{\partial x} x^{\alpha -1} e^{-x}=
(\alpha-1) x^{\alpha-2}e^{-x}-x^{\alpha -1}e^{-x}
$$
Put this equation equal to $0$ and we get

$$
(\alpha -1) x^{\alpha -2} e^{-x}-x^{\alpha -1} e^{-x}=0 \implies
e^{-x}x^{\alpha-1} \cdot((\alpha-1)x^{-1}-1)
$$

$$
\implies x=\alpha-1.
$$
The supremum for $\alpha \in (0,1)$ is given by

$$
\text{sup}_{x} f^{*}(x)=(\alpha-1)^{\alpha-1}e^{\alpha-1}
\implies 
a=\sqrt{(\alpha-1)^{\alpha-1}e^{-\alpha+1}}.
$$
The constant $b_{+}$ is given by
$$
b_{+}=\sqrt{\text{sup}_{x \geq 0}(x^2 f^{*}(x))}.
$$
$$
\frac{\partial}{\partial x}x^2 f^{*}(x)=\frac{\partial}{\partial x}  x^{\alpha +1} e^{-x}=(\alpha+1)x^{\alpha}e^{-x}-e^{-x}x^{\alpha+1}
$$
By setting this equal to zero, we get

$$
0=(\alpha+1)x^{\alpha}e^{-x}-e^{-x}x^{\alpha+1}
\implies x=\alpha+1
$$ 
Thus, the supremum is \begin{equation} \label{eq:gx}
  \text{sup}_{x}x^2f^{*}(x) =
  \begin{cases}
    (\alpha+1)^{\alpha-1} e^{-\alpha-1} & \text{if } 0 < x ,
    \\
    0 & \text{otherwise},
  \end{cases}
\end{equation},
for $x \geq0$,
which means that the constant $b_{+}$ is given by

$$
b_{+}=\sqrt{(\alpha+1)^{(\alpha+1)}e^{-\alpha-1}}.
$$

The constant $b_{-}$ is given by $b_{-}=\sqrt{sup_{x \leq 0}(x^2 f^{*}(x))}=0$, since $f^{*}(x)=0$ when $x \leq0$

b.)

The algorithm to generate $n$ samples from $f$ has to be implemented on log-scale.
The log-transformations are
$$
X_1 \sim \text{Uniform}(0,a) \implies \log(X_1)=\log(a \cdot U_1)=\log(a)+log(U_1)
$$
where $U_1 \sim \text{Uniform}(0,1).$ We also have
$$
X_2 \sim \text{Uniform}(b_{-}, b_{{+}})=\text{Uniform}(0,b_{+}) \implies \log(X_2)=\log(b_{+} \cdot U_2)=\log(b_+)+\log(U_2)
$$
where $U_2 \sim \text{Uniform}(0,1).$ We have 

$$
\frac{x_2}{x_1}=\exp\left(\log\left (\frac{x_2}{x_1} \right)\right)=\exp \left (\log(x_1)+\log(x_2) \right)
$$
\begin{equation} \label{eq:gx}
  f^{*}\left( \frac{x_2}{x_1} \right) =
  \begin{cases}
    \left (\frac{x_2}{x_1} \right)^{\alpha -1}e^{-\left(x_2/x_1 \right)} & \text{if } 0 < x_2/x_1 ,
    \\
    0 & \text{otherwise},
  \end{cases}
\end{equation}
$$
\implies
$$
\begin{equation} \label{eq:gx}
  \log f^{*}\left (\frac{x_2}{x_1} \right ) =
  \begin{cases}
    (\alpha -1) \log(x_2/x_1) -(x_2/x_1) & \text{if } 0 < x_2/x_1 ,
    \\
    0 & \text{otherwise},
  \end{cases}
\end{equation}

\begin{equation} \label{eq:gx}
   =
  \begin{cases}
    (\alpha -1) (\log(x_2)-\log(x_1)) -\exp(\log (x_2)- \log(x_1)) & \text{if } 0 < x_2/x_1 ,
    \\
    0 & \text{otherwise},
  \end{cases}
\end{equation}

We have 
$$
0 \leq x_1 \leq \sqrt{f^{*}(x_2/x_1)} \implies \log(x_1) \implies \frac{1}{2} \log(f^{*}(x_2/x_1)).
$$
The ratio of uniforms method is implemented below.
```{r}
# Function generating samples from f
sample_gamma_2<-function(n,alpha)
{
  #The a constant found in B.2.a
  a<-sqrt((alpha-1)^(alpha-1)*exp(-alpha+1))
  #The b+ constant found in B.2.a
  b<-sqrt((alpha+1)^(alpha+1)*exp(-alpha-1))
  #the logarithm of these constants
  loga <- log(a)
  logb <- log(b)
  #Create an empty vector
  x<-vector(length=n)
  #Count the number of trials
  count<-0
  i=1
  while(i<=n)
  {
      count<-count+1
      log_x1<-loga+log(runif(1))
      log_x2<-logb+log(runif(1))
      #log f*(x2/x1)
      fun_star<-(alpha-1)*(log_x2-log_x1)-exp(log_x2-log_x1)
      #Acceptance step
      if(log_x1<=(1/2)*fun_star)
      {
        x[i]=exp(log_x2-log_x1)
        i<-i+1
      }
  }
  return(list(x=x,count=count))
  }

```

We plot the results for $n=10000$ and $\alpha=30$.
```{r}
n<-10000
alpha<-30
#generate samples with n=10000 and alpha=30
x<-sample_gamma_2(n,alpha)$x
#Plot the density of the generated samples
ggplot()+
  geom_histogram(
    data=as.data.frame(x), mapping=aes(x=x, y=..density..), binwidth=0.7)+
  #Plot the theoretical distribution
  stat_function(fun=dgamma, args=list(shape=alpha), aes(col="Theoretical distribution"))+ggtitle("Simulation of gamma distribution with alpha=30 and beta=1")
```
The theoretical mean and variance are both $30.$ 
```{r}
#Empirical mean and variance
emp_mean_gamma_2<-mean(x)
emp_var_gamma_2<-var(x)
```
The empirical mean and variance are `r emp_mean_gamma_2` and `r emp_var_gamma_2` respectively. Both values are close to the theoretical values. 

Now we want to find out how many tries the algorithm needs to generate $n=1000$ realizations for $\alpha \in (1,100].$
In the code below, the number of trials for $\alpha=2,..,100$ is stored and plotted.

```{r}
n=1000
trials<-vector()
for(i in 2:100)
{
  x<-sample_gamma_2(n,i)$count
  trials[i-1]<-x
}
plot(x=2:100, y=trials, xlab="alpha", ylab="trials", main="Number of trials plotted against different values of alpha")
```


### 3 Simulation of the Gamma distribution

We want to write an R function that generates a vector of $n$ independent samples from a gamma distribution with parameters $\alpha$ and $\beta$. So far we have made functions that generate from a gamma distribution with $\alpha \in (0,1) \ \ \text{and} \ \ \alpha \in(1,\infty)$ and $\beta=1.$ The parameter $\beta$ is an inverse scale parameter. This means that if $X \sim \text{Gamma}(\alpha,1),$ then $\frac{1}{\beta} X \sim \text{Gamma}(\alpha, \beta).$ Thus, when we sample from $\text{Gamma}(\alpha,1),$ we can multiply the samples with $1/\beta.$ The parameter $\alpha$ can also be $1.$ We have already made a function that generate samples from the exponential distribution. If $X \sim \text{Gamma}(1,\beta),$ the $X$ has pdf
$$
f(x)=\beta e^{-\beta x} \implies X \sim\text{exp}(\beta)
$$
Therefore, we can use the function generating from the exponential distribution when $\alpha =1.$

```{r}
sample_gamma_final<-function(n, alpha,beta)
{
  #If alpha=1, we generate from the exponential distribution
  if(alpha==1)
  {
    x<-generate_exponential(n,beta)
  }
  #If alpha>1 we use the function from B.2
  else if(alpha>1)
  {
    x<-(1/beta)*sample_gamma_2(n,alpha)$x
  }
  #If 0<alpha<1, we use the function from B.1
  else if(alpha>0 & alpha<1)
  {
    x<-(1/beta)*sample_gamma(n,alpha)
  }
  else
  {
    return(0)
  }
}
```

We generate realizations for the different cases. The first we do is when $\alpha=3$ and $\beta=2.$ 
```{r}
alpha=3
beta=2
n=10000
sampling_gamma<-sample_gamma_final(n,alpha,beta)

ggplot()+
  geom_histogram(
    data=as.data.frame(sampling_gamma),                     mapping=aes(x=sampling_gamma,y=..density..), binwidth=0.1)+
  xlim(0,5)+
stat_function(
  fun=dgamma,
  args=list(shape=alpha, rate=beta), aes(col="Theoretical distribution")
)+ xlab("x")+ ggtitle("Simulation of Gamma distribution with alpa=3 and beta=2")
```

We try an example for when $\alpha=1 \ \ \text{and} \ \ \beta=2$

```{r}
alpha=1
beta=2
n=10000
sampling_gamma<-sample_gamma_final(n,alpha,beta)
#Plotting
ggplot()+
  geom_histogram(
    data=as.data.frame(sampling_gamma),                     mapping=aes(x=sampling_gamma,y=..density..), binwidth=0.1)+
  xlim(0,4)+
stat_function(
  fun=dgamma,
  args=list(shape=alpha, rate=beta), aes(col="Theoretical distribution")
)+xlab("x")+ggtitle("Simulation of Gamma distribution with alpa=1 and beta=2")
```
At last, we try when $\alpha=0.5$ and $\beta=3$

```{r}
alpha=0.5
beta=3
n=10000
sampling_gamma<-sample_gamma_final(n,alpha,beta)
#Plotting
ggplot()+
  geom_histogram(
    data=as.data.frame(sampling_gamma),                     mapping=aes(x=sampling_gamma,y=..density..), binwidth=0.01)+
  xlim(0,5)+xlim(0,2)+ylim(0,6)+
stat_function(
  fun=dgamma,
  args=list(shape=alpha, rate=beta), aes(col="Theoretical distribution")
)+ xlab("x")+ ggtitle("Simulation of gamma distribution with alpha=0.5 and beta=3")
```

We calculate the empirical mean and variance in the last example.
```{r}
emp_mean_gamma_3<-mean(sampling_gamma)
emp_var_gamma_3<-var(sampling_gamma)
```
The empirical mean and variance are `r emp_mean_gamma_3` and `r emp_var_gamma_3` respectively. The theoretical values are 
$E(X)=\alpha / \beta=1/6$ and $Var(X)=\alpha / \beta^2=1/18,$ which means that our estimates are close to the theoretical values. 

### 4 Sampling of Beta distribution


Let $x \sim \text{Gamma}(\alpha,1)$ and $y \sim \text{Gamma}(\beta,1)$ be independent 
and let $z=x/(x+y)$. The pdfs of $X$ and $Y$ are
$$
f_{X}(x)=\frac{x^{\alpha-1}e^{-x}}{ \Gamma (\alpha)}
$$
$$
f_{Y}(y)=\frac{y^{\beta-1} e^{-y}}{\Gamma (\beta)}
$$
The joint distribution of $X$ and $Y$ is given by

$$
f_{X,Y}(x,y)=f_{x}(x) \cdot f_y(y)=\frac{x^{\alpha-1}y^{\beta-1}e^{-x-y}}{\Gamma(\beta) \Gamma(\alpha)}.
$$
Let $Z=\frac{X}{X+Y}$ and $V={X+Y},$ which means that we use the transformations
$$
x=h_1(z,v)=z\cdot v \ \ \text{and} \ \ y=h_2(z,v)=v(1-z).
$$
The Jacobian is
$$
J= \left |
\begin{matrix}
\frac{\partial x}{\partial z}& \frac{\partial x}{\partial v} \ \\
\frac{\partial y}{\partial z} & \frac{\partial y}{\partial v}
\end{matrix} \right|=\frac{\partial x}{\partial z} \frac{\partial y}{\partial v}-\frac{\partial y}{\partial z} \frac{\partial x}{\partial v}
$$
$$
=v(1-z)-(-v)\cdot z=v
$$
The joint distribution of $U$ and $V$ is given by
$$
f_{Z,V}(z,v)=f_{X,Y}(h_1(z,v),h_2(z,v))|J|
$$

$$
=f_{x,y}(z \cdot v, v(1-z)) \cdot v=\frac{(z \cdot v)^{\alpha -1}(v(1-z))^{\beta -1} e^{-zv -v+vz}}{\Gamma(\alpha)\Gamma(\beta)}\cdot v
$$
$$
=\frac{v^{\alpha+\beta-1}z^{\alpha -1}(1-z)^{\beta-1}e^{-v}}{\Gamma(\alpha) \Gamma(\beta)}
$$
The distribution of $V=X+Y$ will be a gamma distribution since $X$ and $Y$ are independent.
The mgf of V is given by
$$
M_V(t)=E[e^{Vt}]=E[e^{(X+Y)t}]=E[e^{Xt}] \cdot E[e^{Yt}]=M_x(t) \cdot M_y(t)
$$
$$
=\left (\frac{1}{1-t} \right )^{\alpha} \cdot \left (\frac{1}{1-t} \right)^{\beta}=\left (\frac{1}{1-t} \right )^{\alpha +\beta}.
$$
$$
\implies V \sim \text{Gamma}(\alpha +\beta,1)
\implies f_{V}(v)=\frac{v^{\alpha+\beta-1}e^{-v}}{\Gamma(\alpha+\beta)}
$$
Since $Z$ and $V$ are independent, the joint distribution can be written.

$$
f_{Z,V}(z,v)=f_{Z}(z) \cdot f_V(v) \implies
f_Z(z)=\frac{f_{Z,V}(z,v)}{f_{V}(v)}
$$
$$
\implies 
f_Z(z)=\frac{v^{\alpha+\beta-1}z^{\alpha -1}(1-z)^{\beta-1}e^{-v}}{\Gamma(\alpha) \Gamma(\beta)} \cdot \frac{\Gamma(\alpha + \beta)}{v^{\alpha +\beta -1 }e^{-v}}=\frac{\Gamma(\alpha + \beta)z^{\alpha-1}(1-z)^{1-\beta}}{\Gamma(\alpha) \Gamma(\beta)}
$$
This is the density of a $\text{beta}(\alpha, \beta)$ distribution. 

(b)
This means that we can use the function for generating samples from a $\text{Gamma}(\alpha, \beta)$ to sample from $x \sim \text{Gamma}(\alpha, 1)$ and $y \sim \text{Gamma}(\beta,1)$
and return $z=x/(x+y)$.
In the following code, $n$ independent samples from a beta function is generated.

```{r}
#Sample from a beta distribution
sample_from_beta <-function(n,alpha,beta)
{
  #first sample from Gamma(alpha, 1)
  x<-sample_gamma_final(n,alpha,1)
  #Second sample from Gamma(beta,1)
  y<-sample_gamma_final(n,beta,1)
  #return z
  z<-x/(x+y)
  return(z)
}
```
We generate sample from the function and plot it together with the theoretical distribution with $\alpha=3$ and $\beta=2$.

```{r}
alpha=3
beta=2
sampling_beta<-sample_from_beta(n, alpha, beta)
#plotting
ggplot()+
  geom_histogram(
    data=as.data.frame(sampling_beta),
    mapping=aes(x=sampling_beta, y=..density..),
    binwidth=0.01
  )+
  stat_function(fun=dbeta, args=list(shape1=alpha,shape2=beta), aes(col="Theoretical distribution"))+ggtitle("Simulation from the Beta distribution with alpha=3 and beta=2")
```

The theoretical mean and variance is given by
$$
E(X)=\frac{\alpha}{\alpha + \beta}=0.6 \ \ \text{and} \ \ \text{Var}(X)=\frac{\alpha \beta}{(\alpha+\beta)^2(\alpha +\beta+1)}=0.04
$$
```{r}
emp_mean_beta<-mean(sampling_beta)
emp_var_beta<-var(sampling_beta)
```
The emirical mean and variance are `r emp_mean_beta` and `r emp_var_beta` respectively. 

## Problem C: Monte Carlo integration and variance reduction

Here we will consider Monte Carlo integration to estimate $\theta = P(X>4)$ when $X \sim N(0,1)$. Then we will compare the variance reduction in importance sampling and antithethic sampling.

### 1 Monte Carlo integration {#C1}
Let $h(X) = I(X>4)$, where $I$ is the indicator function, such that
$$
\begin{split}
  E[h(X)] &= \int^{\infty}_{-\infty} h(x) f(x) dx \\
  &= \int^{\infty}_{-\infty} I(x>4)f(x)dx \\
  &= P(X>4) \\
  &= \theta.
\end{split}
$$
Then, the Monte Carlo Estimate of $\theta$ is given by
$$
\hat \theta_{MC} = \frac1n \sum^{n}_{i=1} h(x_i).
$$
Now we will find a $1-\alpha$ confidence interval for $\theta$ based on our sample set. First we need the expected value of the Monte Carlo estimator
$$
\begin{aligned}
  E[\hat \theta] &= E[\frac{1}{n} \sum_{i=1}^n h(x_i)] \\
  &= \frac{1}{n} \sum^{n}_{i=1}{\theta} \\
  &= \theta,
\end{aligned}
$$
and it's variance
$$
\begin{aligned}
  Var(\hat \theta) &= Var(\frac1n \sum^{n}_{i=1}{h(x_i)})  \\
  &= \frac{1}{n^2} \sum^{n}_{i=1}{Var(h(x_i))} \\
  &= \frac1n \frac{1}{n-1} \sum^{n}_{i=1}{(h(x_i) - \hat \theta)^2}.
\end{aligned}
$$
Then we get the statistic
$$
T_{MC} = \frac{\hat \theta_{MC} - \theta}{\sqrt{\hat {Var}(\hat\theta_{MC})}} \sim t_{n-1}.
$$
Below there is an implementation with $n=100000$ samples of $X$ which we use to find the Monte Carlo estimate $\theta$.
```{r C1_MCconfint, options}
set.seed(321)                       # Seed for reproducibility.
n        = 1e5
x        = generate_from_normal(n)  # Drawing n samples fron N(0,1)
h        = x>4
MCest    = mean(h)                  # Monte Carlo estimate
theta    = pnorm(4, lower.tail = F) # True theta
# Confidence interval and results
svMC     = var(h) # Sample variance
alpha    = 0.05
t        = qt(alpha/2, n-1, lower.tail = F) # (1-alpha) significance
lwrUpr   = sqrt(svMC/n)*t # lower and upper deviation from mean
ciMC     = MCest + c(-lwrUpr, lwrUpr)
resultMC = c(Estimator= MCest, 
             Confint  = ciMC, 
             Var      = svMC, 
             error    = abs(theta-MCest))
resultMC
theta
```
Here we see an error in the $1\cdot10^{-5}$ decimal and that the true value coincide with the $95\%$ confidence interval.


### 2 Importance sampling {#C2}
Here we will use importance sampling on the same problem as in [C1](#C1) to try to reduce the variance of the Monte Carlo integration. The proposal distribution is
$$
g(x) = 
\begin{cases}
  cxe^{-x^2/2} &, \ x>4 \\
  0 &,\ \text{otherwise},
\end{cases}
$$
where $c$ is a normalizing constant. Now, let $x_1,...,x_n {\sim} g(x)$ and let $w_i = f(x_i)/g(x_i)$ be the weights. Then, the importance sampling estimator of $\theta$ is
$$
\hat \theta_{IS} = \frac{ \sum^{n}_{i=1}{h_i w_i}}{n}. 
$$
In order to use inversion sampling on the proposal distribution $g$ we need its cdf,
$$
\begin{split}
  G(x) &= \int^{x}_{4}{cye^{-y^2/2}}dy \\ 
  &= \int^{x^2/2}_{8}{ce^{-u}du} \\
  &=\left[ -ce^{-u}\right]_{8}^{x^2/2} \\
  &= c(e^{-8} - e^{-x^2/2}).
\end{split}
$$
Since $g$ is a distribution, and therefore $\int^{\infty}_{4}{g(x)}dx = 1$,  we can find $c$ by solving
$$
\begin{split}
  \left. c(e^{-8} - e^{-x^2/2}) \right |_{x=\infty} &=1\\
  c &= e^{8}.
\end{split}
$$
Then we have $G(x) = 1 - e^{8-x^2/2}$. Now we can sample from $g$ by solving $U=G(x)\sim Unif(0,1)$ for $x$, that is,
$$
\begin{split}
  U &= 1 - e^{8-x^2/2} \\
  -2ln(1-U) &= x^2 - 16 \\
  x &= \sqrt{16 - 2ln(1-U)}.
\end{split}
$$
Thus, our samples are generated by inserting randomly selected $U\sim Unif(0,1)$ admits samples from $X\sim g$.

We also need the expected value,
$$
\begin{split}
  E[\hat \theta_{IS}] &= E \left[\frac{ \sum^{n}_{i=1}{h_i w_i}}{n} \right] 
  \\
  &= \frac{1}{n} \sum^{n}_{i=1}{ \int^{\infty}_{0}{h_i \frac{f_i}{g_i} g_i dx}} 
  \\
  &= \frac1n \sum^{n}_{i=1} \int^{\infty}_{0} h_i f_i dx \\
  &= \frac1n \sum^{n}_{i=1} E[h_i] \\
  &= \frac1n n\theta \\
  &= \theta,
\end{split}
$$
and the sample variance,
$$
\begin{split}
  Var(\hat \theta_{IS}) &= Var \left(\frac{ \sum^{n}_{i=1}{h_i w_i}}{n} \right) 
  \\
  &= \frac{1}{n(n-1)} \sum^{n}_{i=1} \left( h_i w_i - \sum^{n}_{i=1} \frac{h_iw_i}{n} \right)^2 
  \\
  &= \frac{1}{n(n-1)} \sum^{n}_{i=1}\left( h_i w_i - \hat \theta_{IS}\right)^2,
\end{split}
$$
of the importance sample estimator to compute the $(1-\alpha)%$ confidence interval. Below we have implemented the computation of the importance sample estimate along with the $95\%$ confidence interval.
```{r expIS, options}
expSampler <- function(n){
  # Samples from proposal distribution g
  u = runif(n)
  return(sqrt(16-2*log(1-u)))
}
w <- function(x) {
  # Weight function
  f = dnorm(x)
  g = ifelse(x>4, x*exp(8-0.5*x^2),0)
  return(f/g)
}
set.seed(321)
gx        = expSampler(n) # Sample from proposal
gh        = (gx>4)*1
ISest     = mean(gh*w(gx)) # Importance sample estimate
svIS      = var(gh*w(gx))
ISconfint = ISest + c(-t*sqrt(svIS/n), t*svIS/n)
resultIS  = c(ISestimate=ISest, confint=ISconfint, var=svIS, error = abs(theta - ISest))
results   = rbind(MC=resultMC, IS=resultIS)
results
theta
```
Here we see that importance sampling has reduced the variance by a factor of $Var(\hat\theta_{MC}) / Var(\hat \theta_{IS}) =$ `svMC/svIS` $=$ `r svMC/svIS`. Also, the importance sample estimator is a much more precise estimate considering the error. If we let variance be a more general proxy for precision, then we can estimate the number of samples $n$ needed in [C1](#C1) to obtain the same precision as the importance sampling technique with $m=100000$ samples. This is done by setting
$$
\begin{split}
  Var(\hat \theta_{MC}) &= Var(\hat \theta_{IS}) \\
  \Leftrightarrow \frac1nVar(h(X)) &= \frac1m Var(h(X)w(X)) \\
  \Leftrightarrow n &= m \frac{Var(h(X))}{Var(h(X)w(X))}.
\end{split}
$$
From the previous results we get the estimated number of samples, $n =$ `svMC / svIS` $=$ `r n*svMC / svIS`, needed to obtain the same precision as we did using importance sampling as a variance reducing technique.  

### 3 Antithetic Sampling
Now we will combine the importance sampling with the use of antithetic variates. We start by modifying the sample generator for $g$ in task [C2](#C2) so that it produces $n$ pairs, $X=x_i$ and $Y=y_i$, of antithetic variates generated from $G^{-1}$ with inputs $u_i$ and $1-u_i$, respectively.
```{r C3antitheticSampler, options}
expSamplerAnti <- function(n) {
  # Pairwise sampling from proposal distribution
  # evaluated at u and 1-u.
  u = runif(n) 
  return(data.frame(x=sqrt(16-2*log(1-u)),
                    y=sqrt(16-2*log(u))))
}
```
Then, the importance sample estimates for each of the pairs are
$$
\begin{split}
  \hat \theta_X &= \frac1n \sum^{n}_{i=1} h(x_i)w(x_i),\\
  \hat \theta_Y &= \frac1n \sum^{n}_{i=1} h(y_i)w(y_i),
\end{split}
$$
and the antithetic sample estimator is
$$
\hat \theta_A = \frac{\hat \theta_X + \hat \theta_Y}{2}. 
$$
We also need the expected value
$$
\begin{split}
  E[\hat \theta_{AS}] &= E\left[ \frac{\hat \theta_X + \hat \theta_Y}{2} \right]
  \\
  &= \frac{1}{2n} \sum^{n}_{i=1} (E[h(x_i)w(x_i)] + E[h(y_i)w(y_i)]) \\
  &= \frac{1}{2n} \sum^{n}_{i=1} 2\theta \\
  &= \theta,
\end{split}
$$
and the variance is
$$
\begin{split}
  Var(\hat \theta_{AS}) &= \frac14(Var(\hat \theta_X) + Var(\hat \theta_Y) + 2Cov(\hat \theta_X,\hat \theta_Y))
\end{split}
$$
where $\rho_{XY}=Cov(\hat \theta_X,\hat \theta_Y)$ and $S^2_{XY}$ is the sample variance of either estimator $\hat\theta_X$ or $\hat\theta_Y$. For a fair comparison to importance sampling we set the number of samples $m = n/2$ since antithetic sampling generates $m$ samples for each of the function evaluations of $G^{-1}(\cdot)$.
```{r C3Antithetic, options}
set.seed(321)
m         = 5e4
xy        = expSamplerAnti(m)
hx        = (xy$x>4)*1 # h(X)
hy        = (xy$y>4)*1 # h(Y)
hwx       = hx*w(xy$x) # h(X)w(X)
hwy       = hy*w(xy$y) # h(Y)w(Y)
AS        = (hwx + hwy)/2 # (theta_x + theta_y)/2
ASest     = mean(AS)
svAS      = var(AS)
lwrUprAS  = c(-t,t)*sqrt(svAS/n)
confintAS = ASest + lwrUprAS
resultAS  = c(ASest, confintAS, svAS, abs(theta - ASest))
rbind(results, AS=resultAS)
```
Here we see an even lower variance and slightly better estimate with regards to the error.

## Problem D: Rejection sampling and importance sampling

### 1.
The observed data is $y=[y_1,y_2,y_3,y_4]=[125,18,20,34]$


The multinomial mass function is $f(y|\theta) \propto (2+\theta)^{y_1}(1-\theta)^{y_2+y_3} \theta^{y_4}.$ Assuming a uniform prior, the posterior will be 
$$
f(\theta | y) \propto f^{*}(\theta)=(2+\theta)^{y_1}(1-\theta)^{y_2+y_3}\theta^{y_4} \ \ \theta \in (0,1)
$$
We construct a rejection sampling algorithm to simulate from $f(\theta|y).$ The proposal distribution is $g(\theta)=1.$ The constant $c$ should satisfy 
$$
f(\theta|y) \leq c \cdot g(\theta|y)=c
$$
We find the maximum of $f(\theta | y)$ given the observed values. 

```{r}
#f-posterior
f_posterior <-function(theta)
{
  f<-(2+theta)^125*(1-theta)^(18+20)*theta^(34)
  return(f)
}
```


```{r}
#Find the c-values
c<-optimize(f_posterior, c(0,1), maximum = TRUE)$objective
```

```{r}
#rejection samping
rejection_sampling_f <-function(n){
  x<-vector()
  num_gen=0
for(i in 1:n)
{
  finished=0
while(finished==0){
  #Generate form U(0,1)
  xi<-runif(1)
  #Acceptance probability
  alpha<-(1/c)*f_posterior(xi)
  #Generate for u
  u<-runif(1)
  #Accept for x
  if(u<=alpha)
  {
    x[i]=xi
    finished=1
  }
  num_gen<-num_gen+1
}  
  
}
  return(list(sample_f=x,numbers=num_gen))
}
```


### 2.

We want to estimate the posterior mean of $\theta$ by Monte-Carlo integration using $M=10000$ samples from $f(\theta | y).$ The Monte-carlo estimate of the mean is given by
$$
\hat{\mu}=\frac{1}{M} \sum_{i=1}^{M} \Theta_i
$$
where $\Theta_i, ...,\Theta_M \sim f(\theta|y)$. 

We find the normalizing $k$ for the posterior, which is found by solving the following equation for $k$
$$
1=k\int_{0}^{1} f^{*}(\theta) d\theta=\int_{0}^{1} (2+\theta)^{y_1}(1-\theta)^{y_2+y_3}\theta^{y_4} d \theta
$$
$$
\implies k=\frac{1}{\int_{0}^{1}(2+\theta)^{y_1}(1-\theta)^{y_2+y_3} \theta^{y_4} d \theta} 
$$
In the code below, a histogram of the sample is drawn and is compared to the theoretical distribution. The estimated posterior mean of $\theta$ is also found and compared to the theoretical value. The estimated mean is plotted as a line in the plot below.
```{r}
#normalizing constant
norm_con<-integrate(f_posterior,0,1)$val
#The exact posterior
posterior <-function(theta)
{
  return((1/norm_con)*f_posterior(theta))
}
```


```{r}
#Generate samples
M<-10000
theta_sampling<-rejection_sampling_f(M)$sample_f

#Plot the
mu_hat<-mean(theta_sampling)
mu_hat

mu_theoretical<-integrate(function(theta)(theta*posterior(theta)),0,1)$val
mu_theoretical
ggplot()+
  geom_histogram(data=as.data.frame(theta_sampling),          mapping=aes(x=theta_sampling,y=..density..),
                 binwidth = 0.01)+
stat_function(
  fun=posterior, aes(col="Theoretical distribution")
)+ xlab("Theta")+ geom_vline(aes(xintercept=mu_hat, col="Estimated posterior mean"))
```


The estimated posterior mean of $\theta$ using Monte-Carlo integration is `r mu_hat`. The theoretical value of the mean using numerical integration is `r mu_theoretical`.

### 3.
The number of random numbers the sampling algorithm needs to generate on average is given by the total number of random numbers the the algorithm generates divided by the number of samples of $f(\theta|y)$

```{r}
total_number<-rejection_sampling_f(M)$numbers
num<-total_number/10000
```
The amount of random numbers needed to generate to obtain one sample of $f(\theta|y)$ is `r num`. The overall acceptance probability is given by
$$
P(U \leq \frac{1}{c} \cdot\frac{f(X)}{g(X)})=\int_{-\infty}^{\infty} dx= \frac{f(x)}{c \cdot g(x)}g(x)=c^{-1}
$$
This means that $c$ is the expected number of tries to obtain one sample of $f(\theta|y),$ which is `r c/norm_con`. This number is close to the number from our implemented algorithm.


## 4. Beta$(1,5)$ as prior.


In section [D2](#D2) we used a uniform prior for $\theta$. Now we will assume a Beta$(1,5)$ prior instead, that is
$$
\begin{split}
  f(\theta) &= \frac{1}{B(1,5)} \theta^{1-1}(1-\theta)^{5-1} \\
  &= \frac{1}{B(1,5)} (1-\theta)^4,
\end{split}
$$
where $B$ is the beta function. Then, the posterior is
$$
\begin{split}
  f(\theta|y) &= \frac{(2+\theta)^{y_1}(1-\theta)^{y_2+y_3}\theta^{y_4}(1-\theta)^4/B(1,5)}
  {\int_0^1(2+\theta)^{y_1}(1-\theta)^{y_2+y_3}\theta^{y_4}(1-\theta)^4/B(1,5)d\theta}
  \\
  &\propto (2+\theta)^{y_1}(1-\theta)^{y_2+y_3 + 4}\theta^{y_4},
\end{split}
$$
which is the un-scaled posterior denoted $f^*(\theta|y)$. As the proposal distribution we use the un-scaled posterior from section [D2](#D2), $f^*(\theta)$, such that the importance sample weights are
$$
w_i = \frac{f^*(\theta_i|y)}{f^*(\theta_i)} = (1-\theta_i)^4
$$
The self-normalizing importance sample estimator is then
$$
\tilde \mu_{IS} =  \frac{\sum^{n}_{i=1} \theta_i w_i}{ \sum^{n}_{i=1} w_i} .
$$
```{r D4beta15prior, options}
f_posterior15 <- function(theta){
  # Posterior when prior is beta(1,5)
  return(f_posterior(theta) * (1-theta)^4)
}
norm_con15 = integrate(f_posterior15, 0, 1)$val
c<-optimize(f_posterior15, c(0,1), maximum = TRUE)$objective
posterior15 <- function(theta){
  # Normalized posterior
  return(f_posterior15(theta)/norm_con15)
}
w <- function(x){
  return((1-x)^4)
}
impSamp = theta_sampling*w(theta_sampling)
# Self-normalized IS estimate
impEst = sum(impSamp)/sum(w(theta_sampling)) 
# Theoretical mean
mu_theoretical15 = integrate(function(theta) (theta*posterior15(theta)),0,1)$val
c(IS = impEst, Theoretical = mu_theoretical15)
rbind(Estimate = c(B15 = impEst, B11 = mu_hat),
      NumericalInt = c(mu_theoretical15,mu_theoretical))
```
Here we see the `B15` column corresponding to our estimated mean using $B(1,5)$ as prior with its numerically integrated value below, and the computations from section [D2](#D2) in the `B11` column. We see that the former estimated mean coincide well with the corresponding numerical integration. Also we notice that these values are smaller than those in column `B11`, which can be explained by the importance sample weight function $w_i = (1-\theta_i)^4$ favouring small value of $\theta \in (0,1)$.
